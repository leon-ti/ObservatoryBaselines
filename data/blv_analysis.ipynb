{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9493e21d-db7a-4428-8e25-a13c29b578aa",
   "metadata": {},
   "source": [
    "# Codes for my Bachelorthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580c92-ea71-42e4-82aa-6ea1faacefc3",
   "metadata": {},
   "source": [
    "## Read the data from the provided blv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8411ebd-d8b8-4802-bb3e-622433cc28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crucial imports to make the codes work\n",
    "# the blv analysis works with Magpy, an open source programm to deal with geomagnetic data\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'/path/to/magpy/') # should be path to magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core.methods import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbf2a7e-70cb-4ae4-9b88-9074bfa48874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'aae', 'abg', 'abk', 'aia', 'ale', 'ams', 'api', 'aqu', 'ars', 'asc', 'asp', 'bdv', 'bel', 'bfe', 'bfo', 'blc', 'bmt', 'bng', 'bou', 'box', 'brd', 'brw', 'bsl', 'cbb', 'cki', 'clf', 'cmo', 'cnb', 'cpl', 'csy', 'cta', 'cyg', 'czt', 'ded', 'dlr', 'dlt', 'dmc', 'dou', 'drv', 'dur', 'ebr', 'esk', 'eyr', 'fcc', 'frd', 'frn', 'fur', 'gan', 'gck', 'gdh', 'gln', 'gna', 'gng', 'gua', 'gui', 'gzh', 'had', 'hbk', 'her', 'hlp', 'hon', 'hrb', 'hrn', 'hua', 'hyb', 'ipm', 'iqa', 'irt', 'izn', 'jai', 'jco', 'kak', 'kdu', 'kep', 'khb', 'kiv', 'kmh', 'kny', 'kou', 'ler', 'lnp', 'lon', 'lov', 'lrm', 'lvv', 'lyc', 'lzh', 'mab', 'maw', 'mbc', 'mbo', 'mcq', 'mea', 'mgd', 'mid', 'mmb', 'naq', 'nck', 'new', 'ngk', 'nur', 'nvs', 'orc', 'ott', 'paf', 'pag', 'pbq', 'peg', 'pet', 'phu', 'pil', 'ppt', 'pst', 'qsb', 'res', 'sba', 'sbl', 'sfs', 'she', 'shu', 'sit', 'sjg', 'sod', 'spg', 'spt', 'stj', 'stt', 'sua', 'tam', 'tan', 'tdc', 'teo', 'thl', 'thy', 'trw', 'tsu', 'ttb', 'tuc', 'ups', 'val', 'vic', 'vna', 'vos', 'vss', 'wic', 'wng', 'yak', 'ykc']\n"
     ]
    }
   ],
   "source": [
    "# identify all observatory codes with blv files\n",
    "\n",
    "destination = '/path/to/blvdata/' #change path to folder with all blv files\n",
    "\n",
    "obscodelist = []\n",
    "\n",
    "# Walk through the directory tree, searching for files\n",
    "for root, dirs, files in os.walk(destination):\n",
    "    for file in files:\n",
    "        # Check if the file has the \".blv\" extension\n",
    "        if file.endswith(\".blv\"):\n",
    "             # Extract the first 3 characters of the filename (assumed to be the observatory code)\n",
    "             obscodelist.append(file[:3])\n",
    "obscodelist = sorted(list(set(obscodelist)))\n",
    "print (obscodelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b2253-8b84-4927-a180-b269869d35e9",
   "metadata": {},
   "source": [
    "## Plot the observed basevalues and the adopted baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa1b70-c0e4-4aa3-bb36-f5fa15f3768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single observatory (i.e. ott)\n",
    "\n",
    "data = read(os.path.join(destination, 'ott*'), mode= None)\n",
    "dataad = read(os.path.join(destination, 'ott*'), mode= 'adopted')\n",
    "print(len(data), data.header.get(\"DataComponents\"))\n",
    "mp.tsplot([data, dataad],     \n",
    "          [['dx','dy','dz']], \n",
    "          symbols=[['.', '.', '.'],['-','-','-']], \n",
    "          title=data.header.get('StationID'), \n",
    "          padding=[[2,0.005,2]], \n",
    "          symbolcolor=['grey','red'], \n",
    "          height=2)\n",
    "# Optionally save the plot as an image (commented out here)\n",
    "#plt.savefig(\"/path/to/plot_folder/ott.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7157b-0f19-457a-b256-5d287d1bbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the baselines for all observatory in obscodelist with for-loop\n",
    "\n",
    "for el in obscodelist:\n",
    "    data = read(os.path.join(destination, '{}*'.format(el)), mode=mode)\n",
    "    dataad = read(os.path.join(destination, '{}*'.format(el)), mode= 'adopted')\n",
    "    print(len(data), data.header.get(\"DataComponents\"))\n",
    "    mp.tsplot([data, dataad],[['dx','dy','dz']], symbols=[['.', '.', '.'],['-','-','-']], title=data.header.get('StationID'), padding=[[2,0.005,2]], symbolcolor=['grey','red'], height=2)\n",
    "    # Optionally save the plot as an image (commented out here)\n",
    "    #plt.savefig(\"/path/to/plot_folder/mixed_baselines/{}.png\".format(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd75b6-71d5-4ed0-afcd-654bda864602",
   "metadata": {},
   "source": [
    "## Adjust adopted data\n",
    "\n",
    "- correct jumps in data with adding the difference of the values before and after a jump to every value after the jump\n",
    "- add a fitting function (polynomial with degree 2) to cancel out bigger variations, linear increase, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa76fe-e612-4f03-af55-e42cb5e0a710",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f316f-cc06-4d07-805b-df03b63935d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jump and drift correction in a single method\n",
    "\n",
    "def correct_jumps(data, jumps=True, drift=True, debug=False):\n",
    "    \"\"\"\n",
    "    Identifying and correcting jumps\n",
    "    \"\"\"\n",
    "\n",
    "    # Identifying and correcting jumps  \n",
    "    # ----------------------------------------\n",
    "    # Step 1\n",
    "    # Using the first derivative to find steep changes in the data set asociated with jumps.\n",
    "    # Indices are determined for all three baseline components. Jumps are identified by exceeding\n",
    "    # a certain threshold of the overall standard deviation of the derivative\n",
    "\n",
    "    def drop_sub(l, diff=1):\n",
    "        # remove subsequent indices which differ equal less than diff\n",
    "        index = 1\n",
    "        while index < len(l):\n",
    "            if l[index] - l[index - 1] <= diff:\n",
    "                del l[index]\n",
    "            else:\n",
    "                index += 1\n",
    "        return l\n",
    "\n",
    "    testdata = data.copy()\n",
    "    testdata = testdata.derivative(keys=['dx','dy','dz'],put2keys=['x','y','z'])\n",
    "\n",
    "    inds = []\n",
    "    sensitivity = 1\n",
    "    for idx,comp in enumerate(['x','y','z']):\n",
    "        sensitivity = 1\n",
    "        mx,mdx  = testdata.mean(comp,std=True)\n",
    "        # get indicies of all values exceeding stddev\n",
    "        col = testdata._get_column(comp)\n",
    "        tinds = np.argwhere(np.abs(col) > sensitivity*mdx)\n",
    "        # assume that not more then 10 jumps are present in the timeseries and gradually increase the threshold until less then 10 jumps are found \n",
    "        while len(tinds) > 40:\n",
    "            sensitivity += 1\n",
    "            tinds = np.argwhere(np.abs(col) > sensitivity*mdx)\n",
    "        # add start and end index for the timeranges\n",
    "        nind = [i[0] for i in tinds]\n",
    "        nindstart = [0]\n",
    "        nindend = len(col)-1\n",
    "        nindstart.extend(nind)\n",
    "        nindstart.append(nindend)\n",
    "        # remove subsequent (diff 1) indices\n",
    "        nindstart = drop_sub(nindstart, diff=1)\n",
    "        inds.append(nindstart)\n",
    "    if debug:\n",
    "        print (\"Indices with jumps indications\", inds)\n",
    "\n",
    "    # Step 2\n",
    "    # Combining the jumps to a single list, asuming that jumps affect all components (this is not necessarily true: i.e.\n",
    "    # horizontal rotation of the sensor will only affect x,y but not z)\n",
    "    tempinds = inds[0] + list(set(inds[1]) - set(inds[0]))\n",
    "    totalinds = sorted(tempinds + list(set(inds[2]) - set(tempinds)))\n",
    "    totalinds = drop_sub(totalinds, diff=2)\n",
    "    print (totalinds)\n",
    "\n",
    "    # Create range data [[start1,end1],[start2,end2],...]\n",
    "    end = 0\n",
    "    ar = []\n",
    "    for idx,el in enumerate(totalinds):\n",
    "        if idx > 0:\n",
    "            if end:\n",
    "                start = end + 2\n",
    "            else:\n",
    "                start = 0\n",
    "            end = el\n",
    "            ar.append([start,end])\n",
    "    if debug:\n",
    "        print (\"Combined indices to ranges\", ar)\n",
    "\n",
    "    # Step 3\n",
    "    # Extract specifoc timeranges from data, obtain offsets relative to previous data set and correct\n",
    "    # \n",
    "    tcol = testdata._get_column('time')\n",
    "    newdata = DataStream()\n",
    "    lastx = 0\n",
    "    lasty = 0\n",
    "    lastz = 0\n",
    "    for el in ar:\n",
    "        # define each segment\n",
    "        starttime = tcol[el[0]]\n",
    "        endtime = tcol[el[1]]\n",
    "        tdata = testdata.copy()\n",
    "        tdata = tdata.trim(starttime,endtime)\n",
    "        firstx = tdata._get_column('dx')[0]\n",
    "        firsty = tdata._get_column('dy')[0]\n",
    "        firstz = tdata._get_column('dz')[0]\n",
    "        offx = lastx-firstx\n",
    "        offy = lasty-firsty\n",
    "        offz = lastz-firstz\n",
    "        if debug:\n",
    "            print (\"Absolute offsets at {}: {}={:.2f}{}, {}={:.2f}{}, {}={:.2f}{}\".format(endtime, tdata.header.get('col-dx'), offx, tdata.header.get('unit-col-dx'), tdata.header.get('col-dy'),offy, tdata.header.get('unit-col-dy'), tdata.header.get('col-dz'),offz, tdata.header.get('unit-col-dz')))\n",
    "        # normailze by means\n",
    "        tdata = tdata.offset({'dx': offx,'dy': offy,'dz': offz})\n",
    "        lastx = tdata._get_column('dx')[-1]\n",
    "        lasty = tdata._get_column('dy')[-1]\n",
    "        lastz = tdata._get_column('dz')[-1]\n",
    "        newdata = join_streams(tdata,newdata)\n",
    "    #mp.tsplot(newdata, keys=['dx','dy','dz'])    \n",
    "\n",
    "    # Step 4\n",
    "    if drift:\n",
    "        # Removing non-stationary contribution by fitting a polynomial function\n",
    "        func = newdata.fit(keys=['dx','dy','dz'], fitfunc='poly',degree=2)\n",
    "        #mp.tsplot(newdata, keys=['dx','dy','dz'], functions=[[func,func,func]], height=2)\n",
    "        # and the subtracting these function from the data set\n",
    "        corrdata = newdata.func2stream(func, keys=['dx','dy','dz'],mode='sub')\n",
    "    else:\n",
    "        corrdata = newdata\n",
    "\n",
    "    return corrdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3229fa1-293f-4e51-9e67-a4c9cb68ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application\n",
    "# ---------------------\n",
    "obscode='wic'\n",
    "data = read(os.path.join(destination, \"{}*\".format(obscode)), mode=\"adopted\")\n",
    "corrdata = correct_jumps(data, drift=True, debug=True)\n",
    "mp.tsplot(corrdata, keys=['dx','dy','dz'], height=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef674fd-f315-4272-b30b-a467633d3109",
   "metadata": {},
   "source": [
    "#### Use defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0377b75-2869-4929-ad44-bdd8b121a503",
   "metadata": {},
   "source": [
    "## Find periodicities with a power spectral analysis and Amplitude, DOY Max and Min with the average yearly baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296dd76-a89b-4284-8e19-0cfec7375517",
   "metadata": {},
   "source": [
    "#### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1442dd8-b5ac-4a4a-a1ff-03321708eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# powerspectrum\n",
    "def powerspectrum(trimdata, component='dz', number=0):\n",
    "    # Extract the time column from the provided data\n",
    "    T = trimdata._get_column('time')\n",
    "    print (len(T))\n",
    "    # Create an array of time points, linearly spaced\n",
    "    t = np.linspace(0,len(T),len(T))\n",
    "    # Extract the signal data (default component is 'dz')\n",
    "    h = trimdata._get_column(component)\n",
    "    # Get the sampling rate\n",
    "    sr = trimdata.samplingrate() # in seconds\n",
    "    fs = 1./sr # Calculate the frequency sampling rate (Hz)\n",
    "    # Plot the signal over time in the first subplot\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, layout='constrained')\n",
    "    ax0.plot(t, h)\n",
    "    ax0.set_xlabel('Time')\n",
    "    ax0.set_ylabel('Signal')\n",
    "    # Calculate the power spectrum using the periodogram (psd function)\n",
    "    power, freqs = psd(h, NFFT=len(t), pad_to=len(t), Fs=fs, detrend='mean', scale_by_freq=True)\n",
    "    plt.xscale('log') # logarithmic scale\n",
    "    plt.show()\n",
    "    # Import functions to find peaks in the power spectrum\n",
    "    from scipy.signal import find_peaks, peak_prominences\n",
    "    # Plot the power spectrum with logarithmic scale for both axes\n",
    "    fig = plt.figure()\n",
    "    plt.plot(freqs, power)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    # Find prominent peaks in the power spectrum (within the first 500 frequencies)\n",
    "    minprom=np.max(power)-np.max(power)*0.90 # Set minimum prominence threshold\n",
    "    peaks, _=find_peaks(power[:500], prominence=(minprom,None))\n",
    "    # Calculate the period of the identified peaks (in days)\n",
    "    period=1/freqs[peaks]/86400\n",
    "    print(period)\n",
    "    plt.title(\"Powerspectrum for {} with periodicities: {}\".format(code[number], period))\n",
    "    plt.plot(freqs[peaks], power[peaks], 'x')\n",
    "    # Optionally save the plot as an image (commented out here)\n",
    "    #plt.savefig(\"/path/to/plot_folder/power/{}.png\".format(code[number]))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28cef1-8b8a-4a9f-a9c7-54993a2da3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and display the average yearly baseline for a given component\n",
    "def show_yearly_average(data, component='dy', show=True, number=0, code=None):\n",
    "    # Define the range of years (from 1990 to 2023)\n",
    "    years = np.arange(1990,2024,1)\n",
    "    full = []\n",
    "    # Loop over each year in the specified range\n",
    "    for year in years:\n",
    "        st = \"{}-01-01\".format(year)\n",
    "        et = \"{}-01-01\".format(year+1)\n",
    "        # Trim the data to the current year (extract the data for this year)\n",
    "        trimdata = data.trim(st,et)\n",
    "        # Get the specified column (default 'dy') from the trimmed data\n",
    "        valcol = trimdata._get_column(component)\n",
    "        doy = np.arange(1,366,1)\n",
    "        # Check if the length of the column matches the expected number of days (365 or 366)\n",
    "        if len(valcol) in [365,366]:\n",
    "            if len(valcol) == 366:\n",
    "                # Drop 29 February\n",
    "                valcol = np.asarray(list(valcol[:59])+ list(valcol[60:]))\n",
    "             # Append the adjusted values (mean-centered) for this year to the list\n",
    "            full.append(valcol-np.mean(valcol))\n",
    "    # Calculate the average and standard deviation across all years (ignoring NaNs)\n",
    "    m = np.nanmean(full, axis=0)\n",
    "    s = np.nanstd(full, axis=0)\n",
    "    # Find the day of the year with the maximum and minimum average values\n",
    "    doymax = np.argmax(m)\n",
    "    doymin = np.argmin(m)\n",
    "    # Get the corresponding values for the max and min days\n",
    "    maxval = np.max(m)\n",
    "    minval = np.min(m)\n",
    "    # Check if the amplitude (difference between max and min) is valid\n",
    "    amplitude_valid = m[doymax] - s[doymax] > m[doymin] + s[doymin]\n",
    "\n",
    "    # If amplitude is valid, print the results\n",
    "    if amplitude_valid:\n",
    "        print(f\"Max: Day {doymax}, Value {maxval}\")\n",
    "        print(f\"Min: Day {doymin}, Value {minval}\")\n",
    "    else:\n",
    "        print(\"No clear amplitude detected.\")\n",
    "        maxval = \"none\"\n",
    "        minval = \"none\"\n",
    "    \n",
    "    # Save the results (max and min days and values) to a CSV file\n",
    "    results = {\n",
    "        'Day of Max': [doymax],\n",
    "        'Day of Min': [doymin],\n",
    "        'Amplitude Max': [maxval],\n",
    "        'Amplitude Min': [minval]\n",
    "    }\n",
    "    #save_results_to_csv(f\"/path/to/csv_folder/to/save/csv_file/{code}_yearly_average.csv\", results)\n",
    "\n",
    "    # Optionally, plot and show/save the results\n",
    "    if show:\n",
    "        fig = plt.figure()\n",
    "        plt.title(\"Average yearly baseline for {}\".format(code))\n",
    "        plt.plot(doy,m, color='black')\n",
    "        plt.fill_between(doy, m+s, m-s, color='blue', alpha=0.2)\n",
    "        # Optionally save the plot as an image (commented out here)\n",
    "        #plt.savefig(\"/path/to/plot_folder/maxmin/{}.png\".format(code), dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3ec79-5316-48f1-8ada-49bf64ad9b40",
   "metadata": {},
   "source": [
    "#### Use functions for all obervatories in obscodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc169d6-102e-4e3a-945a-ad6e0890d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application\n",
    "# ---------------------\n",
    "keys=['dx','dy','dz']\n",
    "\n",
    "for el in obscodelist:\n",
    "    data = read(os.path.join(destination, \"{}*\".format(el)), mode=\"adopted\")\n",
    "    mp.tsplot(data, keys=['dx','dy','dz'], height=2, symbolcolor=['red'])\n",
    "    plt.close()\n",
    "    data = data.interpolate_nans(['dx','dy','dz'])\n",
    "    corrdata = correct_jumps(data, drift=True, debug=False)\n",
    "    mp.tsplot([corrdata], keys=[['dx','dy','dz']], height=2, symbolcolor=['green'])\n",
    "    #plt.savefig('/path/to/plot_folder/corrected/{}.png'.format(el))\n",
    "    plt.close()\n",
    "    mp.tsplot([data, corrdata], keys=[['dx','dy','dz']], height=2, symbolcolor=['red','green'])\n",
    "    #plt.savefig(\"/path/to/plot_folder/comparsion/{}.png\".format(el))\n",
    "    plt.close()\n",
    "    for key in keys:\n",
    "        analysisdata = corrdata.copy()\n",
    "        powerspectrum(analysisdata, component=key, code=obscode)\n",
    "        show_yearly_average(analysisdata, component=key, show=True, code=obscode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59987cd2-205e-4946-82bd-e222e4afe7bc",
   "metadata": {},
   "source": [
    "## Work with analyzed data\n",
    "\n",
    "### Plot data on a world map\n",
    "\n",
    "#### Import world map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d5de2-8f6a-4d3e-9abb-b292f229d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crucial imports to plot world maps\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import geodatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878814c8-8aeb-404b-995c-0afb05fc5b6c",
   "metadata": {},
   "source": [
    "download geodatasets/ne_110m_land.zip to import worldmap\n",
    "\n",
    "#### Determine longitude and latitude from the observatories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e3cd1-95bb-4ff9-8939-762711afe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV-files\n",
    "df1 = pd.read_csv(\"/path/to/analyzed/data.csv\", delimiter=\";\") # Read the CSV file with the analysed data\n",
    "df2 = pd.read_csv(\"/path/to/iagaList.csv\", delimiter=\";\") # Read the CSV file with observatory information, such as latitude and longitude\n",
    "\n",
    "# Convert the codes in both DataFrames to lowercase to ensure consistent comparison\n",
    "df1['code'] = df1['code'].str.lower()\n",
    "df2['code'] = df2['code'].str.lower()\n",
    "\n",
    "# Merge the two DataFrames based on the 'code' column (matching codes from both files)\n",
    "result = pd.merge(df1, df2, on='code')\n",
    "\n",
    "# Doppelte Eintr√§ge in der ersten Spalte entfernen# Remove duplicate entries in the merged DataFrame based on the 'code' column\n",
    "result = result.drop_duplicates(subset=['code'])\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "#print(result)\n",
    "\n",
    "# Save the merged and cleaned DataFrame to a new CSV file (including both analyzed data and observatory coordinates)\n",
    "result.to_csv('/path/to/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff131798-d68a-4d09-a096-b6a77d3b71ec",
   "metadata": {},
   "source": [
    "#### Plot datasets (CSV files) on the world map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaa16c-889b-4723-af54-42489eb20b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An easy example, how to plot world maps with data. All of my maps are plotted like this, some with more data than other\n",
    "\n",
    "# Read data from CSV files containing the necessary information\n",
    "df = pd.read_csv(\"/path/to/data/i.e./with/periodicity.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "df1 = pd.read_csv(\"/path/to/data/i.e./without/periodicity.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "# Load the world map shapefile for plotting\n",
    "worldmap = gpd.read_file('path/to/geodataset/ne_110m_admin_0_countries.zip')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Plot the basic map of the world with countries outlined\n",
    "worldmap.plot(\n",
    "    ax=ax,\n",
    "    color=\"lightgray\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.5  # Set transparency for better overlay visibility\n",
    ")\n",
    "\n",
    "# Extract longitude and latitude values for the first dataset\n",
    "x = df['Longitude']\n",
    "y = df['Latitude']\n",
    "# Plot the data points for the first dataset (i.e. annual periodicity) in red\n",
    "plt.scatter(x, y,\n",
    "              alpha=0.6,             \n",
    "              c='red',\n",
    "              label='annual periodicity'\n",
    "            )\n",
    "\n",
    "# Extract longitude and latitude values for the second dataset\n",
    "a = df1['Longitude']\n",
    "b = df1['Latitude']\n",
    "# Plot the data points for the second dataset (i.e. no annual periodicity) in blue\n",
    "plt.scatter(a, b,\n",
    "              alpha=0.6,             \n",
    "              c='blue',\n",
    "              label='no annual periodicity'\n",
    "           )\n",
    "\n",
    "# Annotate each data point from the datasets with the corresponding 'code'\n",
    "for i, txt in df.iterrows():\n",
    "    plt.annotate(txt['code'], (x[i], y[i]), fontsize=8, color='red')\n",
    "\n",
    "for i, txt in df1.iterrows():\n",
    "    plt.annotate(txt['code'], (a[i], b[i]), fontsize=8, color='blue')\n",
    "    \n",
    "\n",
    "# Creating axis limits and title\n",
    "plt.xlim([-180, 180])\n",
    "plt.ylim([-90, 90])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Optionally, save the plot to a file (this line is commented out)\n",
    "#plt.savefig('path/to/worldmap/output.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46752e4a-e602-4a3f-a498-c1ac8778e6b0",
   "metadata": {},
   "source": [
    "#### Determine the european observatories\n",
    "(There are many European observatories, and since they are not easily visible on the world map, an additional Europe map will be plotted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba19fce-62e3-42ac-b2f4-d019e1b261a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a CSV file containing the necessary information (assuming it has 'Longitude' and 'Latitude' columns)\n",
    "# The data is stored in 'af', with the file located at the specified path\n",
    "af = pd.read_csv(\"/path/to/analyzed/data.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "# Create a new geometry column by combining the 'Longitude' and 'Latitude' columns into Point objects\n",
    "# This step creates a geometric representation of each location\n",
    "geometry = [Point(xy) for xy in zip(af['Longitude'], af['Latitude'])]\n",
    "\n",
    "# Create a GeoDataFrame (gdf) using the original data ('af') and the new 'geometry' column\n",
    "# A GeoDataFrame is a special kind of DataFrame that handles geometric data\n",
    "gdf = gpd.GeoDataFrame(af, geometry=geometry)\n",
    "\n",
    "# Define the bounding box coordinates that cover the region of Europe\n",
    "# (Longitude between -30 and 50, Latitude between 30 and 75)\n",
    "min_lon, min_lat = -30, 30\n",
    "max_lon, max_lat = 50, 75\n",
    "\n",
    "# Filter the rows of the GeoDataFrame (gdf) that lie within the specified bounding box for Europe\n",
    "# The 'cx' indexer is used to select data within the defined longitude and latitude ranges\n",
    "europe_gdf = gdf.cx[min_lon:max_lon, min_lat:max_lat]\n",
    "\n",
    "# Drop the 'geometry' column from the filtered GeoDataFrame as it's no longer needed for the output\n",
    "# Save the remaining data (excluding the 'geometry' column) to a new CSV file\n",
    "europe_gdf.drop(columns='geometry').to_csv(\"/path/to/EU/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3da0eb-884b-41a8-a686-ca9e41e766ae",
   "metadata": {},
   "source": [
    "#### Plot European Observatories on an European Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f84bd-b894-43ba-9dfd-1ff88b01f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs # Import Cartopy's coordinate reference system (CRS) tools\n",
    "import cartopy.feature as cfeature # Import Cartopy's feature set, which includes borders and coastlines\n",
    "\n",
    "# Reading two CSV files containing data for plotting (including 'Longitude' and 'Latitude')\n",
    "cef = pd.read_csv(\"/path/to/EU/output1.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "cef1 = pd.read_csv(\"/path/to/EU/output2.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "# Define the geographical extent (bounding box) for the map to cover Europe\n",
    "# The extent specifies the limits of the plot in [min_lon, max_lon, min_lat, max_lat]\n",
    "extent = [-20, 40, 33, 70]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# Add a subplot to the figure with a PlateCarree projection (a simple latitude-longitude grid)\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "# Set the map extent based on the defined limits (Europe region)\n",
    "ax.set_extent(extent)\n",
    "\n",
    "# Plot data from the first CSV file (cef) on the map as red points\n",
    "# Extract 'Longitude' and 'Latitude' from the DataFrame to create scatter plot\n",
    "x = cef['Longitude']\n",
    "y = cef['Latitude']\n",
    "plt.scatter(x, y,\n",
    "              alpha=0.6,             \n",
    "              c='red',\n",
    "              label='annual periodicity'\n",
    "            )\n",
    "\n",
    "# Plot data from the second CSV file (cef1) on the map as blue points\n",
    "# Extract 'Longitude' and 'Latitude' from the second DataFrame to create scatter plot\n",
    "a = cef1['Longitude']\n",
    "b = cef1['Latitude']\n",
    "plt.scatter(a, b,\n",
    "              alpha=0.6,             \n",
    "              c='blue',\n",
    "              label='no annual periodicities'\n",
    "           )\n",
    "\n",
    "# Annotate each point from the DataFrames with its corresponding 'code'\n",
    "for i, txt in cef.iterrows():\n",
    "    plt.annotate(txt['code'], (x[i], y[i]), fontsize=10, color='red')\n",
    "\n",
    "for i, txt in cef1.iterrows():\n",
    "    plt.annotate(txt['code'], (a[i], b[i]), fontsize=10, color='blue')\n",
    "\n",
    "\n",
    "# Borders of countries are added with dashed lines and 1px width\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=1)\n",
    "# Coastlines are added to show the outline of landmasses\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "ax.set_xlabel('Longitude')  # x-Achsenbeschriftung\n",
    "ax.set_ylabel('Latitude')   # y-Achsenbeschriftung\n",
    "\n",
    "ax.set_xticks(range(-20, 41, 10))  # Setze x-Achsen-Ticks von -20 bis 40\n",
    "ax.set_yticks(range(33, 76, 10))\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Optionally, save the map to a PNG file (this line is commented out, but can be used if needed)\n",
    "#plt.savefig('/path/to/European/map.png', dpi=fig.dpi)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
